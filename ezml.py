# -*- coding: utf-8 -*-
"""ezml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CFFGCy4_WP7pv2gN7aqp1hJc89HAqXN3
"""

import pandas as pd
from typing import Union, Dict, List, Any
import numpy as np
def map_unique_strings(df: pd.DataFrame,
                       columns: Union[str, List[str]] = 'all',
                       return_mappings: bool = False) -> Union[pd.DataFrame, tuple]:
    """
    Processes DataFrame columns containing string objects by creating and applying unique mappings.

    Parameters:
    -----------
    df : pd.DataFrame
        Input DataFrame to process
    columns : str or list of str, default 'all'
        Columns to process. If 'all', processes all object/string columns
    return_mappings : bool, default False
        Whether to return the mapping dictionaries along with the transformed DataFrame

    Returns:
    --------
    pd.DataFrame or tuple (pd.DataFrame, dict)
        Transformed DataFrame, optionally with mapping dictionaries if return_mappings=True
    """

    # Initialize mapping storage
    mappings = {}

    # Determine which columns to process
    if columns == 'all':
        # Select all object/string columns
        cols_to_process = df.select_dtypes(include=['object', 'string']).columns
    else:
        # Process specified columns (ensure it's a list even if single column provided)
        cols_to_process = [columns] if isinstance(columns, str) else columns

    # Create a copy of the DataFrame to avoid modifying the original
    transformed_df = df.copy()

    for col in cols_to_process:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame")

        # Get unique values and create mapping
        unique_values = transformed_df[col].unique()
        # Create mapping from value to integer code, ignoring NaN values
        value_map = {val: idx for idx, val in enumerate(unique_values) if pd.notna(val)}

        # Apply mapping to the column
        transformed_df[col] = transformed_df[col].map(value_map)

        # Store the mapping if requested
        if return_mappings:
            mappings[col] = value_map

    if return_mappings:
        return transformed_df, mappings
    else:
        return transformed_df

def convert_float_to_int(df: pd.DataFrame,
                        columns: Union[str, List[str]] = 'all') -> pd.DataFrame:
    """
    Converts float columns to integers when all non-NaN values are whole numbers.

    Parameters:
    -----------
    df : pd.DataFrame
        Input DataFrame to process
    columns : str or list of str, default 'all'
        Columns to process. If 'all', processes all float columns

    Returns:
    --------
    pd.DataFrame
        DataFrame with converted columns
    """

    # Make a copy of the DataFrame to avoid modifying the original
    converted_df = df.copy()

    # Determine which columns to process
    if columns == 'all':
        cols_to_process = converted_df.select_dtypes(include=['float']).columns
    else:
        # Process specified columns (ensure it's a list even if single column provided)
        cols_to_process = [columns] if isinstance(columns, str) else columns

    for col in cols_to_process:
        if col not in converted_df.columns:
            raise ValueError(f"Column '{col}' not found in DataFrame")

        # Check if the column is float type
        if pd.api.types.is_float_dtype(converted_df[col]):
            # Check if all non-NaN values are whole numbers
            non_na_values = converted_df[col].dropna()
            if all(non_na_values == non_na_values.astype(int)):
                # Convert to pd.Int64Dtype() which supports NaN
                converted_df[col] = converted_df[col].astype(pd.Int64Dtype())

    return converted_df

def handle_nans(df):
    """
    Process NaN values in a DataFrame:
    - For float columns: Replace NaN with column mean
    - For integer columns: Replace NaN with (max value + 1)
    - Other columns remain unchanged

    Parameters:
    -----------
    df : pd.DataFrame
        Input DataFrame to process

    Returns:
    --------
    pd.DataFrame
        Processed DataFrame with NaN handling
    """

    processed_df = df.copy()

    for column in processed_df.columns:
        dtype = processed_df[column].dtype

        # Handle float columns (including float64, Float64, etc.)
        if pd.api.types.is_float_dtype(dtype):
            col_mean = processed_df[column].mean()
            processed_df[column] = processed_df[column].fillna(col_mean)

        # Handle integer columns (including int64, Int64, etc.)
        elif pd.api.types.is_integer_dtype(dtype):
            # Get max value (ignoring NA)
            max_val = processed_df[column].max()
            # If all values are NA, max_val will be NA - handle this case
            if pd.isna(max_val):
                max_val = 0  # or whatever default you prefer
            # Replace NA with max + 1
            processed_df[column] = processed_df[column].fillna(max_val + 1)
            # Convert back to original dtype
            processed_df[column] = processed_df[column].astype(dtype)

    return processed_df

def nans_look(df):
  import matplotlib.pyplot as plt
  import seaborn as sns
  from yellowbrick.style import set_palette
  set_palette('flatui')
  sns.displot(
      data=df.isnull().melt(value_name='missing'),
      y='variable',
      hue='missing',
      multiple='fill',
      height=8,
      aspect=1.1
  )

  # specifying a threshold value
  plt.axvline(0.4, color='r')
  plt.show()  # Added to actually display the plot

def preprocess_df(df, mapping=False):
  if mapping==False:
    return handle_nans(convert_float_to_int(map_unique_strings(df)))
  else:
    new_df, mappings = map_unique_strings(df, return_mappings=mapping)
    return handle_nans(convert_float_to_int(new_df)), mappings

def encode(
    df: pd.DataFrame,
    mappings: Dict[str, Dict[Any, Any]],
) -> pd.DataFrame:
    """
    Strictly encodes DataFrame columns using provided mappings.

    Args:
        df: Input DataFrame to encode
        mappings: Dictionary of {column_name: {value: encoded_value}} mappings

    Returns:
        Encoded DataFrame with only columns that had mappings

    Raises:
        ValueError: If any column values aren't found in their mapping
    """
    encoded_df = pd.DataFrame(index=df.index)

    for col in df.columns:
        if col not in mappings:
            continue  # Skip columns without mappings

        column_mapping = mappings[col]
        unique_vals = set(df[col].dropna().unique())
        unmapped_vals = unique_vals - set(column_mapping.keys())

        if unmapped_vals:
            raise ValueError(
                f"Column '{col}' contains {len(unmapped_vals)} unmapped values: "
                f"{sorted(unmapped_vals)[:5]}{'...' if len(unmapped_vals) > 5 else ''}"
            )

        encoded_df[col] = df[col].map(column_mapping)

    return encoded_df

import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

def remove_classification_anomalies(df, y, contamination=0.05, random_state=42):
    """
    Remove classification anomalies from a DataFrame based on the target variable.

    Parameters:
    -----------
    df : pd.DataFrame
        Input DataFrame containing features and target variable
    y : str
        Name of the target variable column
    contamination : float (default=0.05)
        Expected proportion of anomalies in the data
    random_state : int (default=42)
        Random seed for reproducibility

    Returns:
    --------
    pd.DataFrame
        DataFrame with anomalies removed
    """

    # Make a copy of the original DataFrame to avoid modifying it
    df_clean = df.copy()

    # Separate features and target
    X = df_clean.drop(columns=[y])
    target = df_clean[y]

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Initialize and fit Isolation Forest for each class
    classes = target.unique()
    anomaly_mask = pd.Series(False, index=df_clean.index)

    for class_label in classes:
        # Get indices of current class
        class_indices = target[target == class_label].index

        # Fit Isolation Forest on this class's data
        clf = IsolationForest(contamination=contamination,
                            random_state=random_state)
        clf.fit(X_scaled[class_indices])

        # Predict anomalies for this class
        class_pred = clf.predict(X_scaled[class_indices])

        # Update anomaly mask (anomalies are marked as -1)
        anomaly_mask.loc[class_indices] = (class_pred == -1)

    # Remove rows marked as anomalies
    df_clean = df_clean[~anomaly_mask]

    return df_clean.reset_index(drop=True)

def decode(
    encoded_df: pd.DataFrame,
    mappings: Dict[str, Dict[Any, Any]],
) -> pd.DataFrame:
    """
    Strictly decodes DataFrame columns using provided mappings (inverse of strict_encode).

    Args:
        encoded_df: Input DataFrame with encoded values to decode
        mappings: Original mapping dictionary {column_name: {original_value: encoded_value}}

    Returns:
        Decoded DataFrame with only columns that had mappings

    Raises:
        ValueError: If any encoded values aren't found in the inverse mapping
    """
    decoded_df = pd.DataFrame(index=encoded_df.index)

    for col in encoded_df.columns:
        if col not in mappings:
            continue  # Skip columns without mappings

        # Create inverse mapping {encoded_value: original_value}
        inv_mapping = {v: k for k, v in mappings[col].items()}

        unique_vals = set(encoded_df[col].dropna().unique())
        unmapped_vals = unique_vals - set(inv_mapping.keys())

        if unmapped_vals:
            raise ValueError(
                f"Column '{col}' contains {len(unmapped_vals)} unmapped encoded values: "
                f"{sorted(unmapped_vals)[:5]}{'...' if len(unmapped_vals) > 5 else ''}"
            )

        decoded_df[col] = encoded_df[col].map(inv_mapping)

    return decoded_df

def corr_matrix(df):
  corr = df.corr()
  return corr.style.background_gradient(cmap='coolwarm')

def preprocess_train_test(train_df, test_df):
  train, maps = preprocess_df(train_df, mapping=True)
  test = encode(test_df, maps)
  return train, test


def solve(model, x_cols, y_col, pred_col_name, train_df_filepath='train.csv', test_df_filepath='test.csv', submission_path='Submission.csv', id_test_col_name='id',
          id_submission_col_name='id', contamination=0.05):
    # Load data
    train_df = pd.read_csv(train_df_filepath)
    test_df = pd.read_csv(test_df_filepath)

    # Preprocess both datasets together to ensure consistent encoding
    combined = pd.concat([train_df[x_cols + [y_col]], test_df[x_cols]], axis=0)
    combined_processed, mappings = preprocess_df(combined, mapping=True)

    # Split back into train and test
    train_processed = combined_processed.iloc[:len(train_df)]
    test_processed = combined_processed.iloc[len(train_df):]

    # Remove anomalies and get matching y values
    train_cleaned = remove_classification_anomalies(train_processed, y_col, contamination=contamination)
    y_train = train_cleaned[y_col].to_numpy()  # Use the y values from cleaned data
    x_train = train_cleaned[x_cols].to_numpy()

    # Train model
    model.fit(x_train, y_train)

    # Make predictions
    x_test = test_processed[x_cols].to_numpy()
    preds = np.array(model.predict(x_test))

    # Create submission
    ids = test_df[id_test_col_name].to_numpy()
    submission = pd.DataFrame({
        id_submission_col_name: ids,
        pred_col_name: preds
    })
    submission.to_csv(submission_path, index=False)
    return submission
